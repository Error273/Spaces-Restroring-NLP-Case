{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Восстановление пропущенных пробелов в тексте с помощью NLP / DL / алгоритма\n",
        "\n",
        "Абраменко Александр Родионович cfif12349@yandex.ru\n",
        "\n",
        "Avito DS Internship 2025 // Тестовое задание 3"
      ],
      "metadata": {
        "id": "d4DD2Qj_2Q2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Я предлагаю следующее решение:\n",
        "\n",
        "1) Разбить исходный текст на части с помощью регулярных выражений, и обрабатывать буквенные и численные символы по отдельности\n",
        "2) Классический алгоритм с частотной оценкой слова. Динамическое программирование + словарь (wordfreq)\n",
        "3) Нейросеть BiLSTM\n",
        "\n",
        "    Для обучения использовать сгенерировать синтетические данные из частотного словаря, а так же реальные данные, размеченные предыдущим алгоритмом\n",
        "4) Beam search для улучшения качества\n",
        "5) Результат будет считаться голосованием по большинству среди предыдущих трех алгоритмов"
      ],
      "metadata": {
        "id": "X_ifzflk20N1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Загрузка данных и установка библиотек"
      ],
      "metadata": {
        "id": "Ya-SqGcl2UM7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lb7swMvbPg5L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# загрузка тестовых данных\n",
        "data = []\n",
        "f = open('dataset_1937770_3.txt').readlines()[1:]\n",
        "for line in f:\n",
        "    id, text = line.strip().split(',', 1)\n",
        "    data.append(text)\n",
        "test_data = pd.Series(data, name='text')\n",
        "test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "fHzoUSvhWJPt",
        "outputId": "616e6347-711d-4819-9819-586f1a02eb5c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                      куплюайфон14про\n",
              "1                   ищудомвПодмосковье\n",
              "2        сдаюквартирусмебельюитехникой\n",
              "3           новыйдивандоставканедорого\n",
              "4                      отдамдаромкошку\n",
              "                     ...              \n",
              "1000                          Янеусну.\n",
              "1001              Весна-яуженегреюпио.\n",
              "1002         Весна-скоровырастеттрава.\n",
              "1003    Весна-выпосмотрите,каккрасиво.\n",
              "1004               Весна-гдемояголова?\n",
              "Name: text, Length: 1005, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>куплюайфон14про</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ищудомвПодмосковье</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>сдаюквартирусмебельюитехникой</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>новыйдивандоставканедорого</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>отдамдаромкошку</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>Янеусну.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1001</th>\n",
              "      <td>Весна-яуженегреюпио.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1002</th>\n",
              "      <td>Весна-скоровырастеттрава.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1003</th>\n",
              "      <td>Весна-выпосмотрите,каккрасиво.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1004</th>\n",
              "      <td>Весна-гдемояголова?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1005 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordfreq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCvuCRc-XMUO",
        "outputId": "f2d294d3-9191-4f18-dc87-1118d4ddaa43"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wordfreq in /usr/local/lib/python3.12/dist-packages (3.1.1)\n",
            "Requirement already satisfied: ftfy>=6.1 in /usr/local/lib/python3.12/dist-packages (from wordfreq) (6.3.1)\n",
            "Requirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.12/dist-packages (from wordfreq) (3.5.0)\n",
            "Requirement already satisfied: locate<2.0.0,>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from wordfreq) (1.1.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from wordfreq) (1.1.1)\n",
            "Requirement already satisfied: regex>=2023.10.3 in /usr/local/lib/python3.12/dist-packages (from wordfreq) (2024.11.6)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy>=6.1->wordfreq) (0.2.13)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes>=3.0->wordfreq) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes>=3.0->wordfreq) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Классический алгоритм"
      ],
      "metadata": {
        "id": "MGWf1Bbm58xP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "from wordfreq import top_n_list, zipf_frequency\n",
        "import re\n",
        "\n",
        "\n",
        "MAX_WORD_LEN = 30\n",
        "LANGS = [\"ru\", \"en\"]\n",
        "UNKNOWN_WORD_DENSITY_PENALTY = -7.0 # штраф за неизвестные словарю слова\n",
        "# награда за более длинное слово,\n",
        "#чтобы алгоритм не выделял короткие, которые являются частями более длинного слова\n",
        "SCORE_LEN_POWER = 1.7\n",
        "\n",
        "def get_word_score(word):\n",
        "    if not word:\n",
        "        return -1e9\n",
        "    word_len = len(word)\n",
        "    len_score = word_len ** SCORE_LEN_POWER\n",
        "    w = word.lower()\n",
        "    best_zipf = max(zipf_frequency(w, lang) for lang in LANGS)\n",
        "    if best_zipf > 0.0:\n",
        "        return best_zipf * len_score\n",
        "    else:\n",
        "        return UNKNOWN_WORD_DENSITY_PENALTY * len_score\n",
        "\n",
        "def segment_one_alpha_token(token):\n",
        "    \"\"\"\n",
        "    получить разбиение строки\n",
        "    \"\"\"\n",
        "    n = len(token)\n",
        "    if n == 0:\n",
        "        return \"\", []\n",
        "    dp = [float(\"-inf\")] * (n + 1)\n",
        "    back = [-1] * (n + 1)\n",
        "    dp[0] = 0.0\n",
        "    for i in range(1, n + 1):\n",
        "        for j in range(max(0, i - MAX_WORD_LEN), i):\n",
        "            word = token[j:i]\n",
        "            score = get_word_score(word)\n",
        "            if dp[j] + score > dp[i]:\n",
        "                dp[i] = dp[j] + score\n",
        "                back[i] = j\n",
        "    if dp[n] == float(\"-inf\"):\n",
        "        return token, []\n",
        "    words = []\n",
        "    indices = []\n",
        "    i = n\n",
        "    while i > 0:\n",
        "        j = back[i]\n",
        "        if i == j:\n",
        "            break\n",
        "        if j > 0:\n",
        "            indices.append(j)\n",
        "        words.append(token[j:i])\n",
        "        i = j\n",
        "    words.reverse()\n",
        "    indices.sort()\n",
        "    return \" \".join(words), indices\n",
        "\n",
        "# разделяет строку на части, где каждая - либо подряд идущие символы,\n",
        "# либо подряд идущие не-символы(числа, знаки)\n",
        "RE_SPLIT = re.compile(r\"([A-Za-zА-Яа-яЁё0-9]+|[^A-Za-zА-Яа-яЁё0-9]+)\", flags=re.UNICODE)\n",
        "\n",
        "def smart_segment_with_indices(text):\n",
        "    \"\"\"\n",
        "    разделение с учетом регулярки\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\", []\n",
        "    parts = RE_SPLIT.findall(text)\n",
        "    out_parts = []\n",
        "    all_indices = []\n",
        "    current_offset = 0\n",
        "    for part in parts:\n",
        "        if re.search(r\"[A-Za-zА-Яа-яЁё]\", part):\n",
        "            segmented_part, local_indices = segment_one_alpha_token(part)\n",
        "            out_parts.append(segmented_part)\n",
        "            for idx in local_indices:\n",
        "                all_indices.append(current_offset + idx)\n",
        "        else:\n",
        "            out_parts.append(part)\n",
        "        current_offset += len(part)\n",
        "    return \"\".join(out_parts), all_indices\n",
        "\n",
        "def smart_segment(text):\n",
        "    res, _ = smart_segment_with_indices(text)\n",
        "    return res\n",
        "\n",
        "\n",
        "def build_wordfreq_dict(langs=('ru','en'), top_k=50000):\n",
        "    \"\"\"\n",
        "    создает частотный словарь из top_k самых частых слов\n",
        "    \"\"\"\n",
        "    wf = {}\n",
        "    for lang in langs:\n",
        "        try:\n",
        "            words = top_n_list(lang, top_k)\n",
        "        except Exception:\n",
        "            words = []\n",
        "        for w in words:\n",
        "            wf[w.lower()] = zipf_frequency(w, lang)\n",
        "    return wf"
      ],
      "metadata": {
        "id": "ib45rMrD6D6Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BiLSTM"
      ],
      "metadata": {
        "id": "gu6UPT2j68hA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "from collections import Counter\n",
        "import json\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, pairs, char2idx):\n",
        "        \"\"\"\n",
        "        pairs - список (string, labels)\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.pairs = pairs\n",
        "        self.pad = \"<PAD>\"\n",
        "        self.unk = \"<UNK>\"\n",
        "        if char2idx is None:\n",
        "            chars = Counter()\n",
        "            for s, _ in self.pairs:\n",
        "                chars.update(list(s))\n",
        "            idx2 = [self.pad, self.unk] + sorted([c for c in chars if c not in (self.pad, self.unk)])\n",
        "            self.idx2char = idx2\n",
        "            self.char2idx = {c:i for i,c in enumerate(idx2)}\n",
        "        else:\n",
        "            self.char2idx = char2idx\n",
        "            idx2 = [None]*len(char2idx)\n",
        "            for c, i in char2idx.items():\n",
        "                 idx2[i] = c\n",
        "            self.idx2char = idx2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s, labels = self.pairs[idx]\n",
        "        ids = [self.char2idx.get(c, self.char2idx.get(self.unk)) for c in s]\n",
        "        return torch.tensor(ids, dtype=torch.long), torch.tensor(labels, dtype=torch.float), s\n",
        "\n",
        "def collate_fn(batch):\n",
        "    ids, labels, raws = zip(*batch)\n",
        "    lengths = [len(x) for x in ids]\n",
        "    maxlen = max(lengths)\n",
        "    pad_id = batch[0][0].new_tensor([0]).item()\n",
        "    ids_p = torch.full((len(ids), maxlen), pad_id, dtype=torch.long)\n",
        "    labels_p = torch.zeros((len(ids), maxlen), dtype=torch.float)\n",
        "    mask = torch.zeros((len(ids), maxlen), dtype=torch.bool)\n",
        "    for i, (id_seq, lab_seq) in enumerate(zip(ids, labels)):\n",
        "        L = id_seq.size(0)\n",
        "        ids_p[i, :L] = id_seq\n",
        "        labels_p[i, :L] = lab_seq\n",
        "        mask[i, :L] = 1\n",
        "    return ids_p, labels_p, mask, raws\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=128, hidden_dim=256, n_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=n_layers, batch_first=True,\n",
        "                            bidirectional=True, dropout=dropout if n_layers>1 else 0.0)\n",
        "        self.fc = nn.Linear(hidden_dim*2, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x, mask=None):\n",
        "        h = self.emb(x)\n",
        "        h = self.dropout(h)\n",
        "        h, _ = self.lstm(h)\n",
        "        logits = self.fc(h).squeeze(-1)\n",
        "        return logits\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, device, bce_loss, grad_clip=1.0):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_masked = 0\n",
        "    for ids_p, labels_p, mask, _ in dataloader:\n",
        "        ids_p = ids_p.to(device)\n",
        "        labels_p = labels_p.to(device)\n",
        "        mask = mask.to(device)\n",
        "\n",
        "        logits = model(ids_p, mask=mask)\n",
        "\n",
        "        logits_masked = logits[mask]\n",
        "        labels_masked = labels_p[mask]\n",
        "\n",
        "        loss = bce_loss(logits_masked, labels_masked)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * labels_masked.size(0)\n",
        "        total_masked += labels_masked.size(0)\n",
        "    return total_loss / (total_masked + 1e-12)\n",
        "\n",
        "def compute_prf_from_flat(probs_flat, gold_flat, mask_flat, threshold):\n",
        "    preds = (probs_flat > threshold).astype(int)\n",
        "    gold = gold_flat.astype(int)\n",
        "    mask = mask_flat.astype(int)\n",
        "\n",
        "    tp = int(((preds == 1) & (gold == 1) & (mask == 1)).sum())\n",
        "    fp = int(((preds == 1) & (gold == 0) & (mask == 1)).sum())\n",
        "    fn = int(((preds == 0) & (gold == 1) & (mask == 1)).sum())\n",
        "    prec = tp / (tp + fp + 1e-12)\n",
        "    rec = tp / (tp + fn + 1e-12)\n",
        "    f1 = 2 * prec * rec / (prec + rec + 1e-12)\n",
        "    return prec, rec, f1, tp, fp, fn\n",
        "\n",
        "def eval_model(model, dataloader, device):\n",
        "    model.eval()\n",
        "    probs_list=[]\n",
        "    gold_list=[]\n",
        "    mask_list=[]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for ids_p, labels_p, mask, _ in dataloader:\n",
        "            ids_p = ids_p.to(device)\n",
        "            labels_p = labels_p.to(device)\n",
        "            mask = mask.to(device)\n",
        "\n",
        "            logits = model(ids_p, mask=mask)\n",
        "            probs = torch.sigmoid(logits)\n",
        "\n",
        "            probs_list.append(probs.cpu().numpy())\n",
        "            gold_list.append(labels_p.cpu().numpy())\n",
        "            mask_list.append(mask.cpu().numpy().astype(int))\n",
        "\n",
        "    probs_flat = np.concatenate([p.reshape(-1) for p in probs_list])\n",
        "    gold_flat = np.concatenate([g.reshape(-1) for g in gold_list])\n",
        "    mask_flat = np.concatenate([m.reshape(-1) for m in mask_list])\n",
        "\n",
        "    probs_masked = probs_flat[mask_flat==1]\n",
        "    gold_masked = gold_flat[mask_flat==1]\n",
        "\n",
        "    if probs_masked.size == 0:\n",
        "        return 0.0,0.0,0.0,0.5\n",
        "\n",
        "    # выбираем лучшее пороговое значение\n",
        "    best_f1 = -1.0\n",
        "    best_t = 0.5\n",
        "    best_p = best_r = 0.0\n",
        "    for thr in np.linspace(0.01,0.99,99):\n",
        "        p, r, f1, *_ = compute_prf_from_flat(probs_masked, gold_masked, np.ones_like(gold_masked), thr)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_t = thr\n",
        "            best_p = p\n",
        "            best_r = r\n",
        "    return best_p, best_r, best_f1, best_t\n",
        "\n",
        "def predict_and_segment_threshold(model, device, raw_text, char2idx, threshold=0.5):\n",
        "    model.eval()\n",
        "    ids = torch.tensor([char2idx.get(c, char2idx.get(\"<UNK>\")) for c in raw_text],\n",
        "                       dtype=torch.long).unsqueeze(0).to(device)\n",
        "    mask = torch.ones_like(ids, dtype=torch.bool).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = model(ids, mask=mask)\n",
        "        probs = torch.sigmoid(logits).squeeze(0).cpu().numpy()\n",
        "    labels = (probs > threshold).astype(int).tolist()\n",
        "    out_chars=[]\n",
        "    for ch, lb in zip(raw_text, labels):\n",
        "        out_chars.append(ch)\n",
        "        if lb==1:\n",
        "            out_chars.append(\" \")\n",
        "    return \"\".join(out_chars), labels, probs\n",
        "\n"
      ],
      "metadata": {
        "id": "0H6QFXyYgqUf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Beam search"
      ],
      "metadata": {
        "id": "FKX_niDB7tNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_segment_from_probs(raw, probs,\n",
        "                            beam_width=20, max_word_len=30, alpha=1.0, beta=1.5):\n",
        "    \"\"\"\n",
        "    score combination: alpha * log(prob_at_boundary) + beta * dp_word_score\n",
        "    \"\"\"\n",
        "    n = len(raw)\n",
        "    beams = {0: [(0.0, [])]}  # pos -> list of (score, seg_list)\n",
        "    for i in range(0, n):\n",
        "        if i not in beams:\n",
        "            continue\n",
        "        states = beams[i]\n",
        "        # перебираем кандидатов\n",
        "        for score, seg in states:\n",
        "            for L in range(1, min(max_word_len, n - i) + 1):\n",
        "                j = i + L\n",
        "                word = raw[i:j]\n",
        "                # вероятность границы после j-1 символа\n",
        "                p_bound = probs[j-1] if (j-1) < len(probs) else 0.0\n",
        "                # избегаем log(0)\n",
        "                log_p = math.log(max(p_bound, 1e-9))\n",
        "\n",
        "                log_w = get_word_score(word)\n",
        "                new_score = score + alpha * log_p + beta * log_w\n",
        "                new_seg = seg + [word]\n",
        "                if j not in beams:\n",
        "                    beams[j] = []\n",
        "                beams[j].append((new_score, new_seg))\n",
        "        #  идем по всем позициям где есть кандидаты\n",
        "        for pos in list(beams.keys()):\n",
        "            if len(beams[pos]) > beam_width:\n",
        "                # выбираем лучших по скору\n",
        "                beams[pos] = sorted(beams[pos], key=lambda x: -x[0])[:beam_width]\n",
        "    if n not in beams: # не нашелся ни один кандидат\n",
        "        return raw\n",
        "    best = max(beams[n], key=lambda x: x[0])\n",
        "    return \" \".join(best[1])\n"
      ],
      "metadata": {
        "id": "v-Tw0QlR70CK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Majority voting"
      ],
      "metadata": {
        "id": "KKfRzyre78Mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seg_to_boundary_vector(seg, raw):\n",
        "    \"\"\"\n",
        "    seg: строка с пробелами\n",
        "    raw: строка без пробелов\n",
        "    возвращает список состоящий из 0/1 где 1 на i-том месте если после него стоит пробел\n",
        "    \"\"\"\n",
        "    vec = [0]*len(raw)\n",
        "    parts = seg.split()\n",
        "    idx = 0\n",
        "    for w in parts:\n",
        "        idx += len(w)\n",
        "        if idx-1 < len(raw):\n",
        "            vec[idx-1] = 1\n",
        "    return vec\n",
        "\n",
        "def majority_vote_segment(cands, raw, threshold_votes=2):\n",
        "    n = len(raw)\n",
        "    votes = [0]*n\n",
        "    for seg in cands:\n",
        "        vec = seg_to_boundary_vector(seg, raw)\n",
        "        for i,v in enumerate(vec):\n",
        "            votes[i] += v\n",
        "    final_vec = [1 if v >= threshold_votes else 0 for v in votes]\n",
        "    out = []\n",
        "    for i,ch in enumerate(raw):\n",
        "        out.append(ch)\n",
        "        if final_vec[i]==1:\n",
        "            out.append(\" \")\n",
        "    return \"\".join(out)\n"
      ],
      "metadata": {
        "id": "XVDE4YLj7_ea"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Разметка"
      ],
      "metadata": {
        "id": "sHxMNMth8MDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pseudo_label_file(file_path, top_percent=0.3, max_pseudo=80000):\n",
        "    pseudo_all=[]\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            s = line.strip()\n",
        "            raw = s.replace(\" \", \"\")\n",
        "\n",
        "            if not raw:\n",
        "                 continue\n",
        "\n",
        "            seg_str, indices = smart_segment_with_indices(raw)\n",
        "            words = [w for w in seg_str.split(\" \") if w != \"\"]\n",
        "            if len(words)==0:\n",
        "                 continue\n",
        "            scores = [get_word_score(w) for w in words]\n",
        "            avg_score = float(sum(scores)/len(scores))\n",
        "            pseudo_all.append((raw, indices, avg_score))\n",
        "\n",
        "    if not pseudo_all:\n",
        "        return []\n",
        "\n",
        "    pseudo_all_sorted = sorted(pseudo_all, key=lambda x: -x[2])\n",
        "    take = min(int(len(pseudo_all_sorted)*top_percent), max_pseudo)\n",
        "    taken = pseudo_all_sorted[:take]\n",
        "    pseudo_pairs = []\n",
        "    for raw, indices, score in taken:\n",
        "        labels = [0] * len(raw)\n",
        "        for idx in indices:\n",
        "            if 0 <= idx < len(raw):\n",
        "                labels[idx-1] = 1\n",
        "        pseudo_pairs.append((raw, labels))\n",
        "    return pseudo_pairs"
      ],
      "metadata": {
        "id": "av4YGezN8Rh7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Конфиг"
      ],
      "metadata": {
        "id": "uWkmdIsr8XoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "N_SYNTHETIC = 120000 # количество синтетических примеров для обучения\n",
        "MAX_WORDS = 6 # максимальное число слов в синтетическом примере\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 12\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SAVE_PATH = \"best_lstm_weights.pt\"\n",
        "META_PATH = SAVE_PATH + \".meta.json\"\n",
        "\n",
        "UPLOADED_PATH = \"dataset_1937770_3.txt\"\n",
        "PSEUDO_TOP_PERCENT = 0.30  # столько процентов топ псевдо лейблов берем\n",
        "MAX_PSEUDO = 60000\n",
        "PSEUDO_WEIGHT = 0.6\n",
        "\n",
        "# beam/voting\n",
        "BEAM_WIDTH = 30\n",
        "BEAM_MAX_WORD_LEN = 30\n",
        "BEAM_ALPHA = 1.0\n",
        "BEAM_BETA = 1.5\n",
        "VOTE_THRESHOLD = 2"
      ],
      "metadata": {
        "id": "Hiy7tUcO8byb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Генерация синтетики"
      ],
      "metadata": {
        "id": "XIE20kV38ioX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_synthetic_sentences(n_sentences=20000, max_words=8, langs=('ru','en')):\n",
        "    \"\"\"\n",
        "    генерирует n_sentences синтетических предложений из популярных слов\n",
        "    и добавляет случайный шум\n",
        "    \"\"\"\n",
        "    words = []\n",
        "    per_lang = max(100000 // len(langs), 1000)\n",
        "    for lang in langs:\n",
        "        words += top_n_list(lang, per_lang)\n",
        "\n",
        "    words = [w for w in words if \" \" not in w and len(w)<=25]\n",
        "\n",
        "    if len(words) < 1000: # если получилось мало слов, то дублируем\n",
        "        words = (words * 10)[:20000]\n",
        "\n",
        "    sentences = []\n",
        "    for _ in range(n_sentences):\n",
        "        k = random.randint(1, max_words)\n",
        "        chosen = random.choices(words, k=k)\n",
        "        for i in range(len(chosen)):\n",
        "            if random.random() < 0.06:\n",
        "                chosen[i] = chosen[i].capitalize() # делаем буквы большими\n",
        "            if random.random() < 0.03: # добавляем число к слову\n",
        "                chosen[i] = chosen[i] + ' ' + str(random.randint(1, 999))\n",
        "            if random.random() < 0.02 and i != len(chosen) - 1:\n",
        "                chosen[i] = chosen[i] + \",\" # ставим запятую\n",
        "\n",
        "        sentence = \" \".join(chosen)\n",
        "        if random.random() < 0.05:\n",
        "            sentence += \".\"\n",
        "        sentences.append(sentence)\n",
        "    return sentences\n",
        "\n",
        "def sentence_to_input_and_labels(sentence):\n",
        "    \"\"\"\n",
        "    взять предложение с пробелами и вернуть предлоэение без пробелов с лейблами\n",
        "    \"\"\"\n",
        "    input_text = sentence.replace(\" \", \"\")\n",
        "    labels = []\n",
        "    for i, ch in enumerate(sentence):\n",
        "        if ch == \" \":\n",
        "            continue\n",
        "        next_is_space = (i + 1 < len(sentence) and sentence[i + 1] == \" \")\n",
        "        labels.append(1 if next_is_space else 0)\n",
        "    assert len(input_text) == len(labels)\n",
        "    return input_text, labels"
      ],
      "metadata": {
        "id": "p6Kli2Qn8pJI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Обучение"
      ],
      "metadata": {
        "id": "EtZE56Gu8xIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Generating synthetic sentences...\")\n",
        "sentences = make_synthetic_sentences(n_sentences=N_SYNTHETIC, max_words=MAX_WORDS)\n",
        "pairs = []\n",
        "for s in sentences:\n",
        "    inp, labs = sentence_to_input_and_labels(s)\n",
        "    if len(inp) > 250:\n",
        "         continue\n",
        "    pairs.append((inp, labs))\n",
        "print(\"Synthetic generated:\", len(pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJBy5Me_9Ak9",
        "outputId": "983e7400-f2e6-4810-e760-5fd02883a722"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating synthetic sentences...\n",
            "Synthetic generated: 120000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pseudo_pairs = []\n",
        "print(\"Pseudo-labeling file...\")\n",
        "pseudo_pairs = pseudo_label_file(UPLOADED_PATH, top_percent=PSEUDO_TOP_PERCENT, max_pseudo=MAX_PSEUDO)\n",
        "print(\"Pseudo selected:\", len(pseudo_pairs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCXuUHQh9An2",
        "outputId": "f40e06d0-ba3a-41e5-ae55-1f3f62b6ba55"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pseudo-labeling file...\n",
            "Pseudo selected: 301\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined = pairs.copy()\n",
        "combined.extend(pseudo_pairs)\n",
        "random.shuffle(combined)\n",
        "n = len(combined)\n",
        "n_train = int(0.85 * n)\n",
        "n_val = int(0.07 * n)\n",
        "\n",
        "train_pairs = combined[:n_train]\n",
        "val_pairs = combined[n_train:n_train+n_val]\n",
        "test_pairs = combined[n_train+n_val:]\n",
        "print(f\"Train/Val/Test sizes: {len(train_pairs)}/{len(val_pairs)}/{len(test_pairs)}\")\n",
        "print(f\"Pseudo in pool: {len(pseudo_pairs)} (weight {PSEUDO_WEIGHT})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fF8yLSbb9AvV",
        "outputId": "4e688aa9-7cc1-409c-9913-ed70942f1de5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train/Val/Test sizes: 102255/8421/9625\n",
            "Pseudo in pool: 301 (weight 0.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = CharDataset(train_pairs, char2idx=None)\n",
        "char2idx = ds_train.char2idx\n",
        "ds_val = CharDataset(val_pairs, char2idx=char2idx)\n",
        "ds_test = CharDataset(test_pairs, char2idx=char2idx)"
      ],
      "metadata": {
        "id": "Mp3UDxDb9HoG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_all_pairs = train_pairs\n",
        "ds_all = CharDataset(ds_all_pairs, char2idx=char2idx)\n",
        "pseudo_set = set([p[0] for p in pseudo_pairs])\n",
        "weights = [PSEUDO_WEIGHT if s in pseudo_set else 1.0 for s, _ in ds_all.pairs]\n",
        "sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)"
      ],
      "metadata": {
        "id": "soJRZIRP9Hq5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(ds_all, batch_size=BATCH_SIZE, sampler=sampler, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(ds_val, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(ds_test, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "oylT40wU9HtX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(ds_train.idx2char)\n",
        "\n",
        "model = BiLSTM(vocab_size=vocab_size, emb_dim=128, hidden_dim=256, n_layers=2, dropout=0.2)\n",
        "model.to(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QbBlW4D9XRp",
        "outputId": "fdf3eade-1387-4472-9385-7ed8a802ff1e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTM(\n",
              "  (emb): Embedding(279, 128, padding_idx=0)\n",
              "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
              "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_pos = sum(sum(l) for _, l in train_pairs) # общее число 1 в трейне\n",
        "num_total = sum(len(s) for s, _ in train_pairs) # всего токенов\n",
        "num_neg = num_total - num_pos # общее число 0\n",
        "pos_w = float(num_neg) / (num_pos + 1e-12) if num_pos > 0 else 1.0 # выбор веса для положительного класса\n",
        "pos_weight = torch.tensor([pos_w], dtype=torch.float, device=DEVICE)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-5)\n",
        "\n",
        "best_val_f1 = -1.0\n",
        "best_thresh = 0.5"
      ],
      "metadata": {
        "id": "E3bhR2Id9XT4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, EPOCHS+1):\n",
        "    t0 = time.time()\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, DEVICE, criterion, grad_clip=1.0)\n",
        "    val_p, val_r, val_f1, val_t = eval_model(model, val_loader, DEVICE)\n",
        "    scheduler.step()\n",
        "    t1 = time.time()\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} | train_loss={train_loss:.6f} | val P={val_p:.4f} R={val_r:.4f} F1={val_f1:.4f} T={val_t:.3f} | time={t1-t0:.1f}s\")\n",
        "    if val_f1 > best_val_f1:\n",
        "        best_val_f1 = val_f1\n",
        "        best_thresh = val_t\n",
        "        torch.save(model.state_dict(), SAVE_PATH)\n",
        "        meta = {\"char2idx\": ds_train.char2idx, \"idx2char\": ds_train.idx2char, \"best_thresh\": float(best_thresh), \"epoch\": epoch, \"val_f1\": float(val_f1), \"pseudo_used\": len(pseudo_pairs)}\n",
        "        with open(META_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "             json.dump(meta, f, ensure_ascii=False)\n",
        "        print(\"Saved best model.\")"
      ],
      "metadata": {
        "id": "7pdtLqab9XWg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d5f0875-759d-4b83-b7f6-4713f9d2774c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12 | train_loss=0.449700 | val P=0.8970 R=0.8559 F1=0.8760 T=0.840 | time=33.8s\n",
            "Saved best model.\n",
            "Epoch 2/12 | train_loss=0.231143 | val P=0.9306 R=0.8679 F1=0.8981 T=0.870 | time=32.0s\n",
            "Saved best model.\n",
            "Epoch 3/12 | train_loss=0.196656 | val P=0.9383 R=0.8820 F1=0.9093 T=0.920 | time=34.1s\n",
            "Saved best model.\n",
            "Epoch 4/12 | train_loss=0.174070 | val P=0.9458 R=0.8874 F1=0.9156 T=0.920 | time=34.7s\n",
            "Saved best model.\n",
            "Epoch 5/12 | train_loss=0.160042 | val P=0.9401 R=0.9004 F1=0.9198 T=0.880 | time=34.1s\n",
            "Saved best model.\n",
            "Epoch 6/12 | train_loss=0.149950 | val P=0.9361 R=0.9087 F1=0.9222 T=0.870 | time=34.0s\n",
            "Saved best model.\n",
            "Epoch 7/12 | train_loss=0.139334 | val P=0.9411 R=0.9120 F1=0.9263 T=0.890 | time=34.3s\n",
            "Saved best model.\n",
            "Epoch 8/12 | train_loss=0.135206 | val P=0.9437 R=0.9131 F1=0.9282 T=0.870 | time=34.6s\n",
            "Saved best model.\n",
            "Epoch 9/12 | train_loss=0.127822 | val P=0.9464 R=0.9142 F1=0.9300 T=0.880 | time=34.2s\n",
            "Saved best model.\n",
            "Epoch 10/12 | train_loss=0.124864 | val P=0.9529 R=0.9118 F1=0.9319 T=0.900 | time=34.3s\n",
            "Saved best model.\n",
            "Epoch 11/12 | train_loss=0.122572 | val P=0.9514 R=0.9127 F1=0.9316 T=0.900 | time=34.0s\n",
            "Epoch 12/12 | train_loss=0.120414 | val P=0.9499 R=0.9152 F1=0.9323 T=0.880 | time=34.6s\n",
            "Saved best model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(SAVE_PATH):\n",
        "    state_dict = torch.load(SAVE_PATH, map_location=DEVICE)\n",
        "    model.load_state_dict(state_dict)\n",
        "if os.path.exists(META_PATH):\n",
        "    with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "        meta = json.load(f)\n",
        "    best_thresh = float(meta.get(\"best_thresh\", best_thresh))"
      ],
      "metadata": {
        "id": "oYPJ5NfW9dR8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wf_dict = build_wordfreq_dict(langs=('ru','en'), top_k=50000)"
      ],
      "metadata": {
        "id": "Q4F0i1MN9dUj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "results = []\n",
        "with torch.no_grad():\n",
        "    for ids_p, labels_p, mask, raws in test_loader:\n",
        "        ids_p = ids_p.to(DEVICE)\n",
        "        logits = model(ids_p, mask=mask.to(DEVICE))\n",
        "        probs_batch = torch.sigmoid(logits).cpu().numpy()\n",
        "\n",
        "        for raw, probs in zip(raws, probs_batch):\n",
        "            raw_str = raw\n",
        "            # первый кандидат\n",
        "            cand_dp = smart_segment(raw_str)\n",
        "            # второй кандидат\n",
        "            labels_thr = (probs > best_thresh).astype(int).tolist()\n",
        "            out_chars = []\n",
        "            for ch, lb in zip(raw_str, labels_thr):\n",
        "                out_chars.append(ch)\n",
        "                if lb == 1:\n",
        "                     out_chars.append(\" \")\n",
        "            cand_nn_thr = \"\".join(out_chars)\n",
        "            # третий кандидат\n",
        "            cand_nn_beam = beam_segment_from_probs(raw_str, probs.tolist(),\n",
        "                                                    beam_width=BEAM_WIDTH,\n",
        "                                                    max_word_len=BEAM_MAX_WORD_LEN,\n",
        "                                                    alpha=BEAM_ALPHA, beta=BEAM_BETA)\n",
        "\n",
        "            final_seg = majority_vote_segment([cand_dp, cand_nn_thr, cand_nn_beam],\n",
        "                                              raw_str, threshold_votes=VOTE_THRESHOLD)\n",
        "            results.append((raw_str, cand_dp, cand_nn_thr, cand_nn_beam, final_seg))\n"
      ],
      "metadata": {
        "id": "gSI-Gk6m9dXP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_preds = []\n",
        "all_golds = []\n",
        "\n",
        "gold_map = {s: labs for s, labs in test_pairs}\n",
        "preds_flat = []\n",
        "golds_flat = []\n",
        "for raw, cand_dp, cand_nn_thr, cand_nn_beam, final_seg in results:\n",
        "    pred_vec = seg_to_boundary_vector(final_seg, raw)\n",
        "    gold = gold_map.get(raw, None)\n",
        "\n",
        "    if gold is None:\n",
        "        continue\n",
        "\n",
        "    preds_flat.extend(pred_vec)\n",
        "    golds_flat.extend(gold)\n",
        "\n",
        "preds_arr = np.array(preds_flat)\n",
        "golds_arr = np.array(golds_flat)\n",
        "tp = int(((preds_arr==1) & (golds_arr==1)).sum())\n",
        "fp = int(((preds_arr==1) & (golds_arr==0)).sum())\n",
        "fn = int(((preds_arr==0) & (golds_arr==1)).sum())\n",
        "prec = tp / (tp + fp + 1e-12)\n",
        "rec = tp / (tp + fn + 1e-12)\n",
        "f1 = 2 * prec * rec / (prec + rec + 1e-12)\n",
        "print(f\"\\nFINAL MAJORITY VOTE TEST => P={prec:.4f} R={rec:.4f} F1={f1:.4f}\")"
      ],
      "metadata": {
        "id": "yxKCG6KK9lQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7449dcb-b1d2-4d9a-840b-a6bdd9f5007d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FINAL MAJORITY VOTE TEST => P=0.6412 R=0.9748 F1=0.7736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "demo = [\n",
        "    \"Аятанцуюпьяныйподдождём\",\n",
        "    \"Курилитолькосиги,есличё\",\n",
        "    \"Нашобразжизнистанетпалачом\",\n",
        "    \"Нискольконежалеюниочём\",\n",
        "    \"Грустно,когдарасписанывсемаршруты\",\n",
        "    \"Когданачасахважныминуты\",\n",
        "    \"куплюайфон14про\",\n",
        "    \"ищудомвПодмосковье\",\n",
        "    \"сдаюквартирусмебельюитехникой\",\n",
        "    \"новыйдивандоставканедорого\",\n",
        "    \"отдамдаромкошку\",\n",
        "    \"работавМосквеудаленно\",\n",
        "    \"куплютелевизорPhilips\",\n",
        "    \"ищугрузчиковдляпереезда\"\n",
        "]\n",
        "for s in demo:\n",
        "    raw = s.replace(\" \", \"\")\n",
        "    cand_dp = smart_segment(raw)\n",
        "    cand_nn_thr, _, probs = predict_and_segment_threshold(model, DEVICE, raw, ds_train.char2idx, threshold=best_thresh)\n",
        "    cand_nn_beam = beam_segment_from_probs(raw, probs.tolist(),\n",
        "                                            beam_width=BEAM_WIDTH, max_word_len=BEAM_MAX_WORD_LEN,\n",
        "                                            alpha=BEAM_ALPHA, beta=BEAM_BETA)\n",
        "    final = majority_vote_segment([cand_dp, cand_nn_thr, cand_nn_beam], raw, threshold_votes=VOTE_THRESHOLD)\n",
        "    print(f\"RAW: {raw}\")\n",
        "    print(\"DP:        \", cand_dp)\n",
        "    print(\"NN_thr:    \", cand_nn_thr)\n",
        "    print(\"NN_beam:   \", cand_nn_beam)\n",
        "    print(\"MAJORITY:  \", final)\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "id": "CmAwH8oz9lTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0361bb62-0f60-4774-fd4a-1918d8704a00"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAW: Аятанцуюпьяныйподдождём\n",
            "DP:         А я танцую пьяный под дождём\n",
            "NN_thr:     Аятанцую пьяный поддождём \n",
            "NN_beam:    Ая танцую пьяный под дождём\n",
            "MAJORITY:   Ая танцую пьяный под дождём \n",
            "---\n",
            "RAW: Курилитолькосиги,есличё\n",
            "DP:         Курили только с и г и,если чё\n",
            "NN_thr:     Курили только сиги, есличё\n",
            "NN_beam:    Курили только сиг и,если чё\n",
            "MAJORITY:   Курили только сиг и,если чё \n",
            "---\n",
            "RAW: Нашобразжизнистанетпалачом\n",
            "DP:         Наш образ жизни станет палачом\n",
            "NN_thr:     Нашобразжизнистанет палачом \n",
            "NN_beam:    Наш образ жизни станет палачом\n",
            "MAJORITY:   Наш образ жизни станет палачом \n",
            "---\n",
            "RAW: Нискольконежалеюниочём\n",
            "DP:         Ни сколько не жалею ни о чём\n",
            "NN_thr:     Нисколько нежале юниочём \n",
            "NN_beam:    Ни сколько не жалею ни о чём\n",
            "MAJORITY:   Ни сколько не жалею ни о чём \n",
            "---\n",
            "RAW: Грустно,когдарасписанывсемаршруты\n",
            "DP:         Грустно,когда расписаны все маршруты\n",
            "NN_thr:     Грустно, когда расписаны всемар шруты\n",
            "NN_beam:    Грустно,когда расписаны все маршруты\n",
            "MAJORITY:   Грустно,когда расписаны все маршруты \n",
            "---\n",
            "RAW: Когданачасахважныминуты\n",
            "DP:         Когда на часа х важны минуты\n",
            "NN_thr:     Когдана часах важныминуты\n",
            "NN_beam:    Когда на часа х важны минуты\n",
            "MAJORITY:   Когда на часа х важны минуты \n",
            "---\n",
            "RAW: куплюайфон14про\n",
            "DP:         куплю айфон 14 про\n",
            "NN_thr:     куплю айфон 14 про\n",
            "NN_beam:    куплю айфон 14 про\n",
            "MAJORITY:   куплю айфон 14 про \n",
            "---\n",
            "RAW: ищудомвПодмосковье\n",
            "DP:         ищу дом в Подмосковье\n",
            "NN_thr:     ищудомв Подмосковье\n",
            "NN_beam:    ищу дом в Подмосковье\n",
            "MAJORITY:   ищу дом в Подмосковье \n",
            "---\n",
            "RAW: сдаюквартирусмебельюитехникой\n",
            "DP:         с даю квартиру с мебелью и техникой\n",
            "NN_thr:     сдаю квартиру смебельюи техникой\n",
            "NN_beam:    сдаю квартиру с мебелью и техникой\n",
            "MAJORITY:   сдаю квартиру с мебелью и техникой \n",
            "---\n",
            "RAW: новыйдивандоставканедорого\n",
            "DP:         новый диван доставка не дорого\n",
            "NN_thr:     новый диван доставка недорого\n",
            "NN_beam:    новый диван доставка не дорого\n",
            "MAJORITY:   новый диван доставка не дорого \n",
            "---\n",
            "RAW: отдамдаромкошку\n",
            "DP:         отдам даром кошку\n",
            "NN_thr:     отдам даром кошку\n",
            "NN_beam:    отдам даром кошку\n",
            "MAJORITY:   отдам даром кошку \n",
            "---\n",
            "RAW: работавМосквеудаленно\n",
            "DP:         работа в Москве удаленно\n",
            "NN_thr:     работав Москве удаленно\n",
            "NN_beam:    работа в Москве удаленно\n",
            "MAJORITY:   работа в Москве удаленно \n",
            "---\n",
            "RAW: куплютелевизорPhilips\n",
            "DP:         куплю телевизор Philip s\n",
            "NN_thr:     куплю телевизор Philips\n",
            "NN_beam:    куплю телевизор Philips\n",
            "MAJORITY:   куплю телевизор Philips \n",
            "---\n",
            "RAW: ищугрузчиковдляпереезда\n",
            "DP:         ищу грузчиков для переезда\n",
            "NN_thr:     ищу грузчиков для переезда\n",
            "NN_beam:    ищу грузчиков для переезда\n",
            "MAJORITY:   ищу грузчиков для переезда \n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Экспорт данных"
      ],
      "metadata": {
        "id": "3zWFPbJY9s6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "def indices_from_segmented(segmented):\n",
        "    \"\"\"\n",
        "    принимает строку с пробелами\n",
        "    возвращает список индексов в строке без пробелов, перед которыми вставляется пробел\n",
        "    \"\"\"\n",
        "    if segmented is None:\n",
        "        return []\n",
        "    parts = [p for p in segmented.strip().split(\" \") if p != \"\"]\n",
        "    indices = []\n",
        "    pos = 0\n",
        "    for i, w in enumerate(parts):\n",
        "        if i == 0:\n",
        "            pos += len(w)\n",
        "            continue\n",
        "        indices.append(pos)\n",
        "        pos += len(w)\n",
        "    return indices"
      ],
      "metadata": {
        "id": "jT_MTG8otP4L"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_res = []\n",
        "for line in test_data:\n",
        "\n",
        "    cand_dp = smart_segment(line)\n",
        "    cand_nn_thr, _, probs = predict_and_segment_threshold(model, DEVICE, line, ds_train.char2idx, threshold=best_thresh)\n",
        "    cand_nn_beam = beam_segment_from_probs(line, probs.tolist(),\n",
        "                                            beam_width=BEAM_WIDTH, max_word_len=BEAM_MAX_WORD_LEN,\n",
        "                                            alpha=BEAM_ALPHA, beta=BEAM_BETA)\n",
        "    final = majority_vote_segment([cand_dp, cand_nn_thr, cand_nn_beam], line, threshold_votes=VOTE_THRESHOLD)\n",
        "    labels_res.append(indices_from_segmented(final))\n",
        "result = pd.DataFrame(columns=['id', 'predicted_positions'])\n",
        "for i in range(len(labels_res)):\n",
        "    result.loc[i] = [i, str(labels_res[i])]\n",
        "result.to_csv('result.csv', index=False)\n"
      ],
      "metadata": {
        "id": "ZhyQOI5CZ_CX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bybpai-uaO-p"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}